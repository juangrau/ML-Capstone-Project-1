{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXLdtr9bO6Jk"
      },
      "source": [
        "# Machine Learning Zoomcamp\n",
        "\n",
        "# Capstone Project 1 - Terrain Image Classification\n",
        "\n",
        "## Just the Training\n",
        "\n",
        "This notebook only contains the final training of the model so anyone can just execute it to obtain the model binaries\n",
        "\n",
        "The dataset is available at Zenodo.org at [this address]('https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1')\n",
        "\n",
        "Lets download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_V3jYYyWwh"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5f4RtwfNzG0",
        "outputId": "365ed4b1-0e67-4b7b-ec16-f7bfb575733b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-30 16:20:45--  https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94658721 (90M) [application/octet-stream]\n",
            "Saving to: ‘EuroSAT_RGB.zip’\n",
            "\n",
            "EuroSAT_RGB.zip     100%[===================>]  90.27M  19.9MB/s    in 6.1s    \n",
            "\n",
            "2024-12-30 16:20:52 (14.7 MB/s) - ‘EuroSAT_RGB.zip’ saved [94658721/94658721]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIO5KuN6FPk8"
      },
      "source": [
        "We extract the data from the zip file downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "co2a42RnXP4G"
      },
      "outputs": [],
      "source": [
        "!unzip -q EuroSAT_RGB.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZQ355l3YAiN"
      },
      "source": [
        "## Data Preparation and split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xm43lR6iXwMP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvhM-53MBqdI",
        "outputId": "0e7a0150-d700-4443-d4ad-4db63839d1c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['River',\n",
              " 'AnnualCrop',\n",
              " 'SeaLake',\n",
              " 'Highway',\n",
              " 'Residential',\n",
              " 'HerbaceousVegetation',\n",
              " 'PermanentCrop',\n",
              " 'Industrial',\n",
              " 'Forest',\n",
              " 'Pasture']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories = os.listdir('EuroSAT_RGB')\n",
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c2XJt_zFgYT"
      },
      "source": [
        "For our model training, we need to create folders for train, test and evaluation.\n",
        "\n",
        "Additionally, as we have 10 different classes of terrain, we need to create folders with these classes inside the training, testing and evaluation forlder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "buxsPSxAUNOK"
      },
      "outputs": [],
      "source": [
        "for dir_name in ['train', 'val', 'test']:\n",
        "    for cat in categories:\n",
        "      os.makedirs(dir_name, exist_ok=True)\n",
        "      os.makedirs(os.path.join(dir_name, cat), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crOo38IQF30z"
      },
      "source": [
        "Now we do the train, test, validation split, meaning that we are making a distribution of the images of each of the 10 classes available, inside the train, test and validation folders and subfolders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSDM6kbhU8y0",
        "outputId": "0220f2db-12b2-4fd3-abfa-b121977ce14c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "River 2500\n",
            "AnnualCrop 3000\n",
            "SeaLake 3000\n",
            "Highway 2500\n",
            "Residential 3000\n",
            "HerbaceousVegetation 3000\n",
            "PermanentCrop 2500\n",
            "Industrial 2500\n",
            "Forest 3000\n",
            "Pasture 2000\n"
          ]
        }
      ],
      "source": [
        "for cat in categories:\n",
        "  image_paths = []\n",
        "  for img in os.listdir(os.path.join('EuroSAT_RGB/', cat)):\n",
        "    image_paths.append(os.path.join('EuroSAT_RGB/', cat, img))\n",
        "  print(cat, len(image_paths))\n",
        "  full_train_paths, test_paths = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "  train_paths, val_paths = train_test_split(full_train_paths, test_size=0.25, random_state=42)\n",
        "  for path in train_paths:\n",
        "    image = path.split('/')[-1]\n",
        "    shutil.copy(path, os.path.join('train', cat, image))\n",
        "  for path in val_paths:\n",
        "    image = path.split('/')[-1]\n",
        "    shutil.copy(path, os.path.join('val', cat, image))\n",
        "  for path in test_paths:\n",
        "    image = path.split('/')[-1]\n",
        "    shutil.copy(path, os.path.join('test', cat, image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnc3Tr3ea6jl"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IT4ctlIffgYP"
      },
      "outputs": [],
      "source": [
        "# Lets import the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR7U2YghIrQ_"
      },
      "source": [
        "### Basic parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Eti7_Npff24E"
      },
      "outputs": [],
      "source": [
        "# Define paths and parameters\n",
        "main_dir = os.getcwd()\n",
        "train_dir = os.path.join(main_dir, 'train')\n",
        "test_dir = os.path.join(main_dir, 'test')\n",
        "val_dir = os.path.join(main_dir, 'val')\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "num_classes = 10  # Adjust based on the number of classes\n",
        "epochs = 35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfnGIzPHI5nh"
      },
      "source": [
        "### Image preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KncJ0ae7Io4t"
      },
      "outputs": [],
      "source": [
        "# Create ImageDataGenerator objects\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.02,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLHZcmJdgjzk",
        "outputId": "09dce9e4-29f6-4545-82e7-2b2c8b301019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 16200 images belonging to 10 classes.\n",
            "Found 5400 images belonging to 10 classes.\n",
            "Found 5400 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53KDe502JAHk"
      },
      "source": [
        "### Make Model function\n",
        "\n",
        "In order to make it easy to test and tune the parameters, we are going to define a function that actually create the model considering the parameters indicated previously (learning rate, dropout rate, data augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AYbovSRrgsez"
      },
      "outputs": [],
      "source": [
        "def make_model(learning_rate, dropout_rate=0.5):\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolutional block\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolutional block (optional, for deeper models)\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H61jATyJL_T4"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YOzrnrdjL-MG"
      },
      "outputs": [],
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    'model_vf_{epoch:02d}_{val_accuracy:.3f}.h5.keras',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN_ZWgjEZOcK",
        "outputId": "492bc809-6a62-40c9-b6ce-2644dc268cbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 195ms/step - accuracy: 0.3913 - loss: 1.6071 - val_accuracy: 0.6337 - val_loss: 1.0361\n",
            "Epoch 2/35\n",
            "\u001b[1m  1/506\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 32ms/step - accuracy: 0.4688 - loss: 1.5053"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4688 - loss: 1.5053 - val_accuracy: 0.6250 - val_loss: 0.9991\n",
            "Epoch 3/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 194ms/step - accuracy: 0.6557 - loss: 0.9433 - val_accuracy: 0.7294 - val_loss: 0.7631\n",
            "Epoch 4/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49us/step - accuracy: 0.5625 - loss: 1.4181 - val_accuracy: 0.7083 - val_loss: 0.7057\n",
            "Epoch 5/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 189ms/step - accuracy: 0.7227 - loss: 0.7747 - val_accuracy: 0.7522 - val_loss: 0.6807\n",
            "Epoch 6/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000us/step - accuracy: 0.8750 - loss: 0.4727 - val_accuracy: 0.7083 - val_loss: 0.6339\n",
            "Epoch 7/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 184ms/step - accuracy: 0.7620 - loss: 0.6695 - val_accuracy: 0.7011 - val_loss: 0.9247\n",
            "Epoch 8/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7188 - loss: 0.8381 - val_accuracy: 0.7500 - val_loss: 0.6183\n",
            "Epoch 9/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 186ms/step - accuracy: 0.7744 - loss: 0.6336 - val_accuracy: 0.7969 - val_loss: 0.5814\n",
            "Epoch 10/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.3268 - val_accuracy: 0.7917 - val_loss: 0.4521\n",
            "Epoch 11/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 187ms/step - accuracy: 0.7954 - loss: 0.5726 - val_accuracy: 0.8238 - val_loss: 0.4973\n",
            "Epoch 12/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.4370 - val_accuracy: 0.8750 - val_loss: 0.5140\n",
            "Epoch 13/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.8190 - loss: 0.5057 - val_accuracy: 0.7489 - val_loss: 0.7190\n",
            "Epoch 14/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.4291 - val_accuracy: 0.8333 - val_loss: 0.5979\n",
            "Epoch 15/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 190ms/step - accuracy: 0.8424 - loss: 0.4538 - val_accuracy: 0.8432 - val_loss: 0.4564\n",
            "Epoch 16/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46us/step - accuracy: 0.8438 - loss: 0.4749 - val_accuracy: 0.7917 - val_loss: 0.6065\n",
            "Epoch 17/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 195ms/step - accuracy: 0.8534 - loss: 0.4309 - val_accuracy: 0.8451 - val_loss: 0.4547\n",
            "Epoch 18/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47us/step - accuracy: 0.8750 - loss: 0.2549 - val_accuracy: 0.8750 - val_loss: 0.2990\n",
            "Epoch 19/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 193ms/step - accuracy: 0.8650 - loss: 0.3875 - val_accuracy: 0.8173 - val_loss: 0.5265\n",
            "Epoch 20/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - accuracy: 0.8438 - loss: 0.3160 - val_accuracy: 0.9167 - val_loss: 0.2272\n",
            "Epoch 21/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 193ms/step - accuracy: 0.8749 - loss: 0.3575 - val_accuracy: 0.8473 - val_loss: 0.4466\n",
            "Epoch 22/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - accuracy: 0.9688 - loss: 0.1189 - val_accuracy: 0.7917 - val_loss: 0.5682\n",
            "Epoch 23/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 190ms/step - accuracy: 0.8843 - loss: 0.3299 - val_accuracy: 0.8385 - val_loss: 0.4893\n",
            "Epoch 24/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48us/step - accuracy: 0.8750 - loss: 0.2754 - val_accuracy: 0.8333 - val_loss: 0.4475\n",
            "Epoch 25/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 194ms/step - accuracy: 0.8841 - loss: 0.3327 - val_accuracy: 0.8651 - val_loss: 0.3794\n",
            "Epoch 26/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51us/step - accuracy: 0.9062 - loss: 0.3022 - val_accuracy: 0.9167 - val_loss: 0.3463\n",
            "Epoch 27/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 194ms/step - accuracy: 0.8897 - loss: 0.3101 - val_accuracy: 0.8661 - val_loss: 0.4107\n",
            "Epoch 28/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.9062 - loss: 0.4345 - val_accuracy: 0.9583 - val_loss: 0.3444\n",
            "Epoch 29/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 199ms/step - accuracy: 0.8980 - loss: 0.2899 - val_accuracy: 0.8339 - val_loss: 0.5112\n",
            "Epoch 30/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.4119 - val_accuracy: 0.9583 - val_loss: 0.2179\n",
            "Epoch 31/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 201ms/step - accuracy: 0.8957 - loss: 0.3037 - val_accuracy: 0.8744 - val_loss: 0.3853\n",
            "Epoch 32/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45us/step - accuracy: 0.9688 - loss: 0.0844 - val_accuracy: 0.7500 - val_loss: 0.6753\n",
            "Epoch 33/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 200ms/step - accuracy: 0.9065 - loss: 0.2772 - val_accuracy: 0.8584 - val_loss: 0.4412\n",
            "Epoch 34/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8438 - loss: 0.3153 - val_accuracy: 0.8333 - val_loss: 0.4659\n",
            "Epoch 35/35\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 195ms/step - accuracy: 0.9144 - loss: 0.2476 - val_accuracy: 0.8687 - val_loss: 0.4138\n",
            "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.8614 - loss: 0.4057\n",
            "Test accuracy: 0.8647693395614624\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "dropout_rate = 0.1\n",
        "batch_size = 32\n",
        "scores = {}\n",
        "\n",
        "model = make_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
        "history = model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch=train_generator.samples // batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_data=validation_generator,\n",
        "  validation_steps=validation_generator.samples // batch_size,\n",
        "  callbacks=[checkpoint])\n",
        "scores['final'] = history.history\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print('Test accuracy:', test_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
